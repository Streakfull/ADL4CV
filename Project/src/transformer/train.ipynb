{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys  # noqadefined\n",
    "import os  # noqa\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '')))  # noqa\n",
    "from termcolor import colored, cprint\n",
    "from transformer.random_tf import RandTransformerModel\n",
    "from options.transformer_options import TransformerOptions\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datasets.shape_net_z_sets import ShapeNetZSets\n",
    "from utils.visualizer import Visualizer\n",
    "import inspect\n",
    "import time\n",
    "from dataset_preprocessing.constants import DATA_SET_PATH, FULL_DATA_SET_PATH\n",
    "from tqdm import tqdm\n",
    "from torch import profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Enc has Attn at i_level, i_block: 3, 0\n",
      "Working with z of shape (1, 256, 8, 8, 8) = 131072 dimensions.\n",
      "[*] Dec has Attn at i_level, i_block: 3, 0\n",
      "[*] VQVAE: weight successfully load from: ../raw_dataset/vqvae.pth\n",
      "---------- Networks initialized -------------\n",
      "-----------------------------------------------\n",
      "[*] create image directory:\n",
      "c:\\Users\\Youssef\\Repos\\TUM\\SS23\\Advanced Deep learning\\ADL4CV\\Project\\src\\transformer\\raw_dataset\\logs\\rand-tf-7\\images...\n"
     ]
    }
   ],
   "source": [
    "vq_cfg = '../configs/pvqae_configs.yaml'\n",
    "tf_config = '../configs/tansformer.yaml'\n",
    "vq_checkpoint = '../raw_dataset/vqvae.pth'\n",
    "\n",
    "options =  TransformerOptions(config_path=vq_cfg, \n",
    "                              tf_config=tf_config,\n",
    "                              vq_ckpt=vq_checkpoint,\n",
    "                              name=\"rand-tf-7\",\n",
    "                              batch_size=3,\n",
    "                              n_epochs=20,\n",
    "                              save_epoch_frequency=1,\n",
    "                              nepochs_decay=5)\n",
    "\n",
    "model = RandTransformerModel()\n",
    "model.initialize(options)\n",
    "visualizer = Visualizer(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"..\"\n",
    "num_works = os.cpu_count()\n",
    "shape_dir = f\"{root_folder}/{DATA_SET_PATH}\"\n",
    "full_dataset_path = f\"{root_folder}/{FULL_DATA_SET_PATH}\"\n",
    "dataset = ShapeNetZSets(shape_dir, cat=\"chairs\",max_data_set_size=10)\n",
    "train_ds, test_ds = torch.utils.data.random_split(\n",
    "    dataset, [0.9, 0.1])\n",
    "\n",
    "train_dl =  DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=options.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True,\n",
    "            num_workers=0)\n",
    "\n",
    "test_dl =  DataLoader(\n",
    "            test_ds,\n",
    "            batch_size=options.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] # training images = 9\n",
      "[*] # testing images = 1\n"
     ]
    }
   ],
   "source": [
    "def get_data_generator(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "test_dg = get_data_generator(test_ds)\n",
    "dataset_size = len(train_ds)\n",
    "\n",
    "\n",
    "cprint('[*] # training images = %d' % len(train_ds), 'yellow')\n",
    "cprint('[*] # testing images = %d' % len(test_ds), 'yellow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Using pytorch's profiler...\n"
     ]
    }
   ],
   "source": [
    "cprint(\"[*] Using pytorch's profiler...\", 'blue')\n",
    "tensorboard_trace_handler = profiler.tensorboard_trace_handler(options.tb_dir)\n",
    "schedule_args = {'wait': 2, 'warmup': 2, 'active': 6, 'repeat': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = profiler.schedule(**schedule_args)\n",
    "activities = [profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(pt_profiler=None,epoch=0):\n",
    "    global total_steps\n",
    "\n",
    "    epoch_iter = 0\n",
    "    for i, data in tqdm(enumerate(train_dl), total=len(train_dl)):\n",
    "        iter_start_time = time.time()\n",
    "        visualizer.reset()\n",
    "        total_steps += options.batch_size\n",
    "        epoch_iter += options.batch_size\n",
    "        model.set_input(data)\n",
    "\n",
    "        model.optimize_parameters(total_steps)\n",
    "\n",
    "        nBatches_has_trained = total_steps // options.batch_size\n",
    "        # if total_steps % options.print_freq == 0:\n",
    "        if nBatches_has_trained % options.print_freq == 0:\n",
    "            errors =model.get_current_errors()\n",
    "\n",
    "            t = (time.time() - iter_start_time) / options.batch_size\n",
    "            visualizer.print_current_errors(\n",
    "                epoch, epoch_iter, total_steps, errors, t)\n",
    "\n",
    "        if (nBatches_has_trained % options.display_freq == 0) or i == 0:\n",
    "            # eval\n",
    "            model.inference(data)\n",
    "            visualizer.display_current_results(\n",
    "               model.get_current_visuals(), total_steps, phase='train')\n",
    "\n",
    "            #model.set_input(next(test_dg))\n",
    "            test_data = next(test_dg)\n",
    "            model.inference(test_data.unsqueeze(0))\n",
    "            visualizer.display_current_results(\n",
    "               model.get_current_visuals(), total_steps, phase='test')\n",
    "\n",
    "        if total_steps % options.save_latest_freq == 0:\n",
    "            cprint('saving the latestmodel (epoch %d, total_steps %d)' %\n",
    "                   (epoch, total_steps), 'blue')\n",
    "            latest_name = f'epoch-latest'\n",
    "            model.save(latest_name)\n",
    "\n",
    "        if pt_profiler is not None:\n",
    "            pt_profiler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Start training. name: rand-tf-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GPU: , epoch: 0, iters: 3, time: 1.551) nll: 6.184075 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*] autoregressively inferencing...:  65%|██████▍   | 332/512 [02:36<01:24,  2.12it/s]\n",
      "  0%|          | 0/3 [02:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Youssef\\Repos\\TUM\\SS23\\Advanced Deep learning\\ADL4CV\\Project\\src\\transformer\\train.ipynb Cell 8\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# epoch_iter = 0\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# profile\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m profiler\u001b[39m.\u001b[39mprofile(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     schedule\u001b[39m=\u001b[39mschedule,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     activities\u001b[39m=\u001b[39mactivities,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     with_stack\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m ) \u001b[39mas\u001b[39;00m pt_profiler:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     train_one_epoch(pt_profiler,epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m options\u001b[39m.\u001b[39msave_epoch_freq \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     cprint(\u001b[39m'\u001b[39m\u001b[39msaving the model at the end of epoch \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, iters \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m            (epoch, total_steps), \u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Youssef\\Repos\\TUM\\SS23\\Advanced Deep learning\\ADL4CV\\Project\\src\\transformer\\train.ipynb Cell 8\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     visualizer\u001b[39m.\u001b[39mprint_current_errors(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         epoch, epoch_iter, total_steps, errors, t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m (nBatches_has_trained \u001b[39m%\u001b[39m options\u001b[39m.\u001b[39mdisplay_freq \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# eval\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     model\u001b[39m.\u001b[39;49minference(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     visualizer\u001b[39m.\u001b[39mdisplay_current_results(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m        model\u001b[39m.\u001b[39mget_current_visuals(), total_steps, phase\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Youssef/Repos/TUM/SS23/Advanced%20Deep%20learning/ADL4CV/Project/src/transformer/train.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m#model.set_input(next(test_dg))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Youssef\\Repos\\TUM\\SS23\\Advanced Deep learning\\ADL4CV\\Project\\src\\transformer\\random_tf.py:284\u001b[0m, in \u001b[0;36mRandTransformerModel.inference\u001b[1;34m(self, data, seq_len, gen_order, topk, prob, alpha, should_render, verbose)\u001b[0m\n\u001b[0;32m    282\u001b[0m tgt_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtgt_pos[:t]\n\u001b[0;32m    283\u001b[0m \u001b[39m# inp_mask = self.generate_square_subsequent_mask(transformer_inp.shape[0], self.opt.device)\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m outp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtf(inp, inp_pos, tgt_pos)\n\u001b[0;32m    285\u001b[0m outp_t \u001b[39m=\u001b[39m outp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[0;32m    286\u001b[0m \u001b[39m# outp_t = F.softmax(outp_t, dim=-1) # compute prob\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Youssef\\Repos\\TUM\\SS23\\Advanced Deep learning\\ADL4CV\\Project\\src\\models\\transformer_networks\\rand_transformer.py:120\u001b[0m, in \u001b[0;36mRandTransformer.forward\u001b[1;34m(self, inp, inp_posn, tgt_posn)\u001b[0m\n\u001b[0;32m    115\u001b[0m inp \u001b[39m=\u001b[39m rearrange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuse_linear(\n\u001b[0;32m    116\u001b[0m     inp), \u001b[39m'\u001b[39m\u001b[39m(t bs) d -> t bs d\u001b[39m\u001b[39m'\u001b[39m, t\u001b[39m=\u001b[39mseq_len, bs\u001b[39m=\u001b[39mbs)\n\u001b[0;32m    118\u001b[0m src_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(seq_len, device)\n\u001b[1;32m--> 120\u001b[0m outp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_transformer(inp, src_mask\u001b[39m=\u001b[39msrc_mask)\n\u001b[0;32m    122\u001b[0m outp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec_linear(outp)\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m outp\n",
      "File \u001b[1;32mc:\\Users\\Youssef\\Repos\\TUM\\SS23\\Advanced Deep learning\\ADL4CV\\Project\\src\\models\\transformer_networks\\rand_transformer.py:90\u001b[0m, in \u001b[0;36mRandTransformer.forward_transformer\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_transformer\u001b[39m(\u001b[39mself\u001b[39m, src, src_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 90\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask)\n\u001b[0;32m     91\u001b[0m     \u001b[39m# output = self.decoder(tgt, memory, tgt_mask=tgt_mask)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:280\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    277\u001b[0m         src_key_padding_mask_for_layers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 280\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[0;32m    282\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    283\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:538\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    536\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    537\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 538\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    539\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    541\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:546\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    545\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 546\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    547\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    548\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    549\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    550\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1167\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   1156\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1157\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   1158\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   1165\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   1166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   1168\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   1169\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   1170\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   1171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1172\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1173\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   1174\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   1175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   1176\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:5046\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   5044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m   5045\u001b[0m     \u001b[39massert\u001b[39;00m in_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 5046\u001b[0m     q, k, v \u001b[39m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[0;32m   5047\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5048\u001b[0m     \u001b[39massert\u001b[39;00m q_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:4737\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[1;34m(q, k, v, w, b)\u001b[0m\n\u001b[0;32m   4734\u001b[0m \u001b[39mif\u001b[39;00m k \u001b[39mis\u001b[39;00m v:\n\u001b[0;32m   4735\u001b[0m     \u001b[39mif\u001b[39;00m q \u001b[39mis\u001b[39;00m k:\n\u001b[0;32m   4736\u001b[0m         \u001b[39m# self-attention\u001b[39;00m\n\u001b[1;32m-> 4737\u001b[0m         \u001b[39mreturn\u001b[39;00m linear(q, w, b)\u001b[39m.\u001b[39mchunk(\u001b[39m3\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   4738\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   4739\u001b[0m         \u001b[39m# encoder-decoder attention\u001b[39;00m\n\u001b[0;32m   4740\u001b[0m         w_q, w_kv \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39msplit([E, E \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "cprint('[*] Start training. name: %s' % options.name, 'blue')\n",
    "total_steps = 0\n",
    "for epoch in range(options.nepochs + options.nepochs_decay):\n",
    "    epoch_start_time = time.time()\n",
    "    # epoch_iter = 0\n",
    "\n",
    "    # profile\n",
    "    with profiler.profile(\n",
    "        schedule=schedule,\n",
    "        activities=activities,\n",
    "        on_trace_ready=tensorboard_trace_handler,\n",
    "        record_shapes=True,\n",
    "        with_stack=True,\n",
    "    ) as pt_profiler:\n",
    "        train_one_epoch(pt_profiler,epoch)\n",
    "\n",
    "    if epoch % options.save_epoch_freq == 0:\n",
    "        cprint('saving the model at the end of epoch %d, iters %d' %\n",
    "               (epoch, total_steps), 'blue')\n",
    "        latest_name = f'epoch-latest'\n",
    "        model.save(latest_name)\n",
    "        cur_name = f'epoch-{epoch}'\n",
    "        model.save(cur_name)\n",
    "\n",
    "    # eval every 3 epoch\n",
    "    if epoch % options.save_epoch_freq == 0:\n",
    "        metrics =model.eval_metrics(test_dl)\n",
    "        visualizer.print_current_metrics(epoch, metrics, phase='test')\n",
    "        print(metrics)\n",
    "\n",
    "    cprint(f'[*] End of epoch %d / %d \\t Time Taken: %d sec \\n%s' %\n",
    "           (\n",
    "               epoch, options.nepochs + options.nepochs_decay,\n",
    "               time.time() - epoch_start_time,\n",
    "               os.path.abspath(os.path.join(options.logs_dir, options.name))\n",
    "           ), 'blue', attrs=['bold']\n",
    "           )\n",
    "    model.update_learning_rate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] weight successfully load from: ./raw_dataset/logs/rand-tf-4/ckpt/rand_tf_epoch-latest.pth\n",
      "BEFORE TRANSFORM\n",
      "First decode\n",
      "Second decode\n"
     ]
    }
   ],
   "source": [
    "tf_checkpoint = './raw_dataset/logs/rand-tf-4/ckpt/rand_tf_epoch-latest.pth'\n",
    "model.load_ckpt(tf_checkpoint)\n",
    "# for i, data in enumerate(train_dl):\n",
    "#     print(data[\"q_set\"].shape)\n",
    "i = dataset[15]\n",
    "i[\"z_set\"] = i[\"z_set\"].argmax(-1).unsqueeze(0)\n",
    "i[\"q_set\"] = i[\"q_set\"].unsqueeze(0)\n",
    "i[\"idx\"]  = i[\"idx\"].unsqueeze(0)\n",
    "model.set_input(i)\n",
    "model.forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
